# Flink 최적화 스트림 처리 구현 체크리스트

> **🎯 핵심 목표**: **역할 분담 최적화** - 복잡한 작업만 Flink, 간단한 작업은 Worker  
> **💡 아키텍처**: Flink 4개 Statement (JOIN/WINDOW 전용) + Worker (단순 처리)

---

## 📊 **최적화된 역할 분담**

### **🔥 Flink 담당 (복잡한 작업)**
- **RTB 재계산**: Campaign JOIN + 알고리즘 계산 + Redis 캐시
- **성과 집계**: 5분 WINDOW + CTR/CVR 계산  
- **전환 매칭**: 24시간 WINDOW + 클릭-전환 JOIN
- **중복 탐지**: 5분 WINDOW + 중복 클릭 탐지 + 보상 이벤트 발행 ⭐ 신규

### **⚡ Worker 담당 (간단한 작업)**
- **잔액 차감**: 클릭 이벤트 → DB INSERT (실시간, 중복 체크 없음)
- **예산 모니터링**: Redis 카운터 업데이트 (실시간)
- **캠페인 상태**: 예산 소진 시 상태 변경 (배치)
- **보상 처리**: compensation-events → DB 잔액 복구 ⭐ 신규

---

## 🚀 Flink Statement 구성 (4개)

### **✅ Statement 1: RTB_CACHE_UPDATER**

```sql
-- 🎯 RTB 순위 계산 및 Redis 캐시 업데이트 (복잡한 JOIN + 알고리즘)

CREATE TABLE campaign_events_source (
  event_type STRING,
  timestamp_field TIMESTAMP(3),
  campaign_id BIGINT,
  advertiser_id BIGINT, 
  product_id BIGINT,
  placement STRING,
  category STRING,
  bid_price DECIMAL(10,2),
  status STRING,
  WATERMARK FOR timestamp_field AS timestamp_field - INTERVAL '30' SECOND
) WITH (
  'connector' = 'kafka',
  'topic' = 'campaign-events',
  'properties.bootstrap.servers' = '${KAFKA_BROKERS}',
  'properties.group.id' = 'flink-rtb-processor',
  'format' = 'json'
);

-- RTB 점수 계산 및 순위 캐시 생성
INSERT INTO rtb_ranking_cache
SELECT 
  placement,
  category,
  COLLECT(
    ROW(
      product_id,
      bid_price * COALESCE(pm.ctr, 0.01) + 0.1 * COALESCE(pm.ctr, 0.01) * COALESCE(pm.cvr, 0.005) * pmd.product_price,
      bid_price,
      campaign_id,
      advertiser_id
    ) ORDER BY (bid_price * COALESCE(pm.ctr, 0.01) + 0.1 * COALESCE(pm.ctr, 0.01) * COALESCE(pm.cvr, 0.005) * pmd.product_price) DESC
  ) as ranking_list,
  COUNT(*) as total_campaigns,
  CURRENT_TIMESTAMP as updated_at

FROM campaign_events_source c
LEFT JOIN product_metrics_cache pm ON c.product_id = pm.product_id
LEFT JOIN product_meta_data pmd ON c.product_id = pmd.product_id
WHERE c.event_type IN ('CAMPAIGN_CREATE', 'CAMPAIGN_UPDATE', 'CAMPAIGN_ACTIVATE')
  AND c.status = 'ACTIVE'
GROUP BY c.placement, c.category;
```

### **✅ Statement 2: PERFORMANCE_AGGREGATOR**

```sql
-- 📊 실시간 성과 집계 (5분 WINDOW + 복잡한 집계)

CREATE TABLE ad_events_source (
  event_type STRING,
  timestamp_field TIMESTAMP(3),
  campaign_id BIGINT,
  advertiser_id BIGINT,
  product_id BIGINT,
  placement STRING,
  category STRING,
  actual_price DECIMAL(10,2),
  revenue_amount DECIMAL(15,2),
  WATERMARK FOR timestamp_field AS timestamp_field - INTERVAL '10' SECOND
) WITH (
  'connector' = 'kafka',
  'topic' = 'ad-events',
  'properties.bootstrap.servers' = '${KAFKA_BROKERS}',
  'properties.group.id' = 'flink-performance-processor',
  'format' = 'json'
);

-- 5분 윈도우 성과 집계
INSERT INTO ad_performance_metrics_realtime
SELECT 
  ROW_NUMBER() OVER (ORDER BY window_start) as id,
  campaign_id,
  product_id,
  placement,
  DATE_FORMAT(window_start, 'yyyy-MM-dd HH:mm:ss') as metric_time,
  
  -- 기본 지표
  SUM(CASE WHEN event_type = 'IMPRESSION' THEN 1 ELSE 0 END) as impression_count,
  SUM(CASE WHEN event_type = 'CLICK' THEN 1 ELSE 0 END) as click_count,
  SUM(CASE WHEN event_type = 'CONVERSION' THEN 1 ELSE 0 END) as conversion_count,
  
  -- 비용/수익
  SUM(CASE WHEN event_type = 'CLICK' THEN actual_price ELSE 0 END) as total_spend,
  SUM(CASE WHEN event_type = 'CONVERSION' THEN revenue_amount ELSE 0 END) as total_revenue,
  
  -- 성과 지표 계산
  CASE 
    WHEN SUM(CASE WHEN event_type = 'IMPRESSION' THEN 1 ELSE 0 END) > 0
    THEN CAST(SUM(CASE WHEN event_type = 'CLICK' THEN 1 ELSE 0 END) AS DOUBLE) 
         / SUM(CASE WHEN event_type = 'IMPRESSION' THEN 1 ELSE 0 END) * 100
    ELSE 0 
  END as ctr,
  
  CASE 
    WHEN SUM(CASE WHEN event_type = 'CLICK' THEN 1 ELSE 0 END) > 0
    THEN CAST(SUM(CASE WHEN event_type = 'CONVERSION' THEN 1 ELSE 0 END) AS DOUBLE) 
         / SUM(CASE WHEN event_type = 'CLICK' THEN 1 ELSE 0 END) * 100
    ELSE 0 
  END as cvr,
  
  CASE 
    WHEN SUM(CASE WHEN event_type = 'CLICK' THEN actual_price ELSE 0 END) > 0
    THEN SUM(CASE WHEN event_type = 'CONVERSION' THEN revenue_amount ELSE 0 END) 
         / SUM(CASE WHEN event_type = 'CLICK' THEN actual_price ELSE 0 END)
    ELSE 0 
  END as roas,
  
  CURRENT_TIMESTAMP as updated_at

FROM TABLE(
  TUMBLE(TABLE ad_events_source, DESCRIPTOR(timestamp_field), INTERVAL '5' MINUTE)
)
WHERE campaign_id IS NOT NULL
GROUP BY campaign_id, product_id, placement, window_start;
```

### **✅ Statement 3: CONVERSION_MATCHER**

```sql
-- 🔗 24시간 클릭-전환 매칭 (복잡한 WINDOW + JOIN)

CREATE TABLE conversion_events_source (
  event_type STRING,
  timestamp_field TIMESTAMP(3),
  device_id STRING,
  session_id STRING,
  user_id STRING,
  order_id STRING,
  product_id BIGINT,
  revenue_amount DECIMAL(15,2),
  WATERMARK FOR timestamp_field AS timestamp_field - INTERVAL '1' HOUR
) WITH (
  'connector' = 'kafka',
  'topic' = 'conversion-events',
  'properties.bootstrap.servers' = '${KAFKA_BROKERS}',
  'properties.group.id' = 'flink-conversion-processor',
  'format' = 'json'
);

-- 24시간 윈도우 클릭-전환 매칭
INSERT INTO conversion_attribution_results
SELECT 
  click_events.ad_id,
  click_events.campaign_id,
  click_events.advertiser_id,
  click_events.product_id,
  conv.order_id,
  conv.revenue_amount,
  TIMESTAMPDIFF(SECOND, click_events.click_time, conv.timestamp_field) as time_to_conversion,
  'LAST_CLICK' as attribution_model,
  click_events.click_time,
  conv.timestamp_field as conversion_time,
  CURRENT_TIMESTAMP as processed_at

FROM (
  -- 클릭 이벤트 24시간 윈도우
  SELECT 
    ad_id,
    campaign_id, 
    advertiser_id,
    product_id,
    device_id,
    session_id,
    timestamp_field as click_time,
    ROW_NUMBER() OVER (
      PARTITION BY device_id, product_id 
      ORDER BY timestamp_field DESC
    ) as click_rank
  FROM TABLE(
    HOP(TABLE ad_events_source, DESCRIPTOR(timestamp_field), INTERVAL '1' HOUR, INTERVAL '24' HOUR)
  )
  WHERE event_type = 'CLICK'
) click_events

INNER JOIN conversion_events_source conv
  ON (conv.device_id = click_events.device_id OR conv.session_id = click_events.session_id)
  AND conv.product_id = click_events.product_id
  AND conv.timestamp_field BETWEEN click_events.click_time AND click_events.click_time + INTERVAL '24' HOUR

WHERE click_events.click_rank = 1  -- Last Click Attribution
  AND conv.event_type = 'CONVERSION';
```

### **✅ Statement 4: DUPLICATE_DETECTOR ⭐ 신규**

```sql
-- 🛡️ 중복 클릭 탐지 및 보상 이벤트 발행 (5분 WINDOW + 복잡한 GROUP BY)

CREATE TABLE ad_events_duplicate_source (
  event_type STRING,
  timestamp_field TIMESTAMP(3),
  device_id STRING,
  session_id STRING,
  user_id STRING,
  ad_id STRING,
  campaign_id BIGINT,
  advertiser_id BIGINT,
  product_id BIGINT,
  actual_price DECIMAL(10,2),
  WATERMARK FOR timestamp_field AS timestamp_field - INTERVAL '30' SECOND
) WITH (
  'connector' = 'kafka',
  'topic' = 'ad-events',
  'properties.bootstrap.servers' = '${KAFKA_BROKERS}',
  'properties.group.id' = 'flink-duplicate-detector',
  'format' = 'json'
);

-- 보상 이벤트 출력 테이블
CREATE TABLE compensation_events_sink (
  timestamp_field TIMESTAMP(3),
  advertiser_id BIGINT,
  campaign_id BIGINT,
  ad_id STRING,
  device_id STRING,
  duplicate_click_count BIGINT,
  refund_amount DECIMAL(10,2),
  original_click_timestamps ARRAY<STRING>,
  detection_window STRING,
  reason STRING,
  compensation_type STRING
) WITH (
  'connector' = 'kafka',
  'topic' = 'compensation-events',
  'properties.bootstrap.servers' = '${KAFKA_BROKERS}',
  'format' = 'json'
);

-- 5분 윈도우 중복 클릭 탐지
INSERT INTO compensation_events_sink
SELECT 
  CURRENT_TIMESTAMP as timestamp_field,
  advertiser_id,
  campaign_id,
  ad_id,
  device_id,
  click_count as duplicate_click_count,
  (click_count - 1) * actual_price as refund_amount,
  click_timestamps as original_click_timestamps,
  '5_MINUTES' as detection_window,
  'DUPLICATE_CLICK_DETECTED' as reason,
  'BALANCE_REFUND' as compensation_type

FROM (
  SELECT 
    advertiser_id,
    campaign_id,
    ad_id,
    device_id,
    actual_price,
    COUNT(*) as click_count,
    COLLECT(DATE_FORMAT(timestamp_field, 'yyyy-MM-dd HH:mm:ss.SSS')) as click_timestamps,
    window_start,
    window_end
    
  FROM TABLE(
    TUMBLE(TABLE ad_events_duplicate_source, DESCRIPTOR(timestamp_field), INTERVAL '5' MINUTE)
  )
  WHERE event_type = 'CLICK'
    AND device_id IS NOT NULL
    AND ad_id IS NOT NULL
  GROUP BY 
    advertiser_id, campaign_id, ad_id, device_id, actual_price, window_start, window_end
  HAVING COUNT(*) > 1  -- 중복 클릭만 필터링
) duplicate_clicks;
```
---

## 🏗️ **최적화된 Terraform 구성**

### **✅ 단일 Flink Job (4개 Statement 통합)**

```hcl
# 🎯 Flink Compute Pool (통합 리소스)
resource "confluent_flink_compute_pool" "rtb_processor" {
  display_name = "rtb-stream-processor"
  cloud        = "AWS"
  region       = "us-west-2"
  max_cfu      = 20
  
  environment {
    id = confluent_environment.main.id
  }
}

# 📊 Statement 1: RTB 캐시 업데이터
resource "confluent_flink_statement" "rtb_cache_updater" {
  compute_pool {
    id = confluent_flink_compute_pool.rtb_processor.id
  }
  principal {
    id = confluent_service_account.flink_runner.id
  }
  
  statement = file("${path.module}/sql/rtb_cache_updater.sql")
  properties = {
    "execution.checkpointing.interval" = "30s"
    "parallelism.default" = "4"
  }
  
  rest_endpoint = confluent_flink_compute_pool.rtb_processor.rest_endpoint
  credentials {
    key    = confluent_api_key.flink_api_key.id
    secret = confluent_api_key.flink_api_key.secret
  }
}

# 📈 Statement 2: 성과 집계
resource "confluent_flink_statement" "performance_aggregator" {
  compute_pool {
    id = confluent_flink_compute_pool.rtb_processor.id
  }
  principal {
    id = confluent_service_account.flink_runner.id
  }
  
  statement = file("${path.module}/sql/performance_aggregator.sql")
  properties = {
    "execution.checkpointing.interval" = "30s"
    "parallelism.default" = "6"
  }
  
  rest_endpoint = confluent_flink_compute_pool.rtb_processor.rest_endpoint
  credentials {
    key    = confluent_api_key.flink_api_key.id
    secret = confluent_api_key.flink_api_key.secret
  }
  
  depends_on = [confluent_flink_statement.rtb_cache_updater]
}

# 🔗 Statement 3: 전환 매칭  
resource "confluent_flink_statement" "conversion_matcher" {
  compute_pool {
    id = confluent_flink_compute_pool.rtb_processor.id
  }
  principal {
    id = confluent_service_account.flink_runner.id
  }
  
  statement = file("${path.module}/sql/conversion_matcher.sql")
  properties = {
    "execution.checkpointing.interval" = "60s"
    "parallelism.default" = "4" 
    "table.exec.state.ttl" = "25h"  # 24시간 + 1시간 버퍼
  }
  
  rest_endpoint = confluent_flink_compute_pool.rtb_processor.rest_endpoint
  credentials {
    key    = confluent_api_key.flink_api_key.id
    secret = confluent_api_key.flink_api_key.secret
  }
  
  depends_on = [confluent_flink_statement.performance_aggregator]
}

# 🛡️ Statement 4: 중복 탐지 & 보상
resource "confluent_flink_statement" "duplicate_detector" {
  compute_pool {
    id = confluent_flink_compute_pool.rtb_processor.id
  }
  principal {
    id = confluent_service_account.flink_runner.id
  }
  
  statement = file("${path.module}/sql/duplicate_detector.sql")
  properties = {
    "execution.checkpointing.interval" = "30s"
    "parallelism.default" = "4"
    "table.exec.state.ttl" = "6h"  # 5분 윈도우 + 1시간 버퍼
  }
  
  rest_endpoint = confluent_flink_compute_pool.rtb_processor.rest_endpoint
  credentials {
    key    = confluent_api_key.flink_api_key.id
    secret = confluent_api_key.flink_api_key.secret
  }
  
  depends_on = [confluent_flink_statement.conversion_matcher]
}
```