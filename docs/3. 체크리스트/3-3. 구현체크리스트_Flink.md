# Flink ìµœì í™” ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

> **ğŸ¯ í•µì‹¬ ëª©í‘œ**: **ì—­í•  ë¶„ë‹´ ìµœì í™”** - ë³µì¡í•œ ì‘ì—…ë§Œ Flink, ê°„ë‹¨í•œ ì‘ì—…ì€ Worker  
> **ğŸ’¡ ì•„í‚¤í…ì²˜**: Flink 4ê°œ Statement (JOIN/WINDOW ì „ìš©) + Worker (ë‹¨ìˆœ ì²˜ë¦¬)

---

## ğŸ“Š **ìµœì í™”ëœ ì—­í•  ë¶„ë‹´**

### **ğŸ”¥ Flink ë‹´ë‹¹ (ë³µì¡í•œ ì‘ì—…)**
- **RTB ì¬ê³„ì‚°**: Campaign JOIN + ì•Œê³ ë¦¬ì¦˜ ê³„ì‚° + Redis ìºì‹œ
- **ì„±ê³¼ ì§‘ê³„**: 5ë¶„ WINDOW + CTR/CVR ê³„ì‚°  
- **ì „í™˜ ë§¤ì¹­**: 24ì‹œê°„ WINDOW + í´ë¦­-ì „í™˜ JOIN
- **ì¤‘ë³µ íƒì§€**: 5ë¶„ WINDOW + ì¤‘ë³µ í´ë¦­ íƒì§€ + ë³´ìƒ ì´ë²¤íŠ¸ ë°œí–‰ â­ ì‹ ê·œ

### **âš¡ Worker ë‹´ë‹¹ (ê°„ë‹¨í•œ ì‘ì—…)**
- **ì”ì•¡ ì°¨ê°**: í´ë¦­ ì´ë²¤íŠ¸ â†’ DB INSERT (ì‹¤ì‹œê°„, ì¤‘ë³µ ì²´í¬ ì—†ìŒ)
- **ì˜ˆì‚° ëª¨ë‹ˆí„°ë§**: Redis ì¹´ìš´í„° ì—…ë°ì´íŠ¸ (ì‹¤ì‹œê°„)
- **ìº í˜ì¸ ìƒíƒœ**: ì˜ˆì‚° ì†Œì§„ ì‹œ ìƒíƒœ ë³€ê²½ (ë°°ì¹˜)
- **ë³´ìƒ ì²˜ë¦¬**: compensation-events â†’ DB ì”ì•¡ ë³µêµ¬ â­ ì‹ ê·œ

---

## ğŸš€ Flink Statement êµ¬ì„± (4ê°œ)

### **âœ… Statement 1: RTB_CACHE_UPDATER**

```sql
-- ğŸ¯ RTB ìˆœìœ„ ê³„ì‚° ë° Redis ìºì‹œ ì—…ë°ì´íŠ¸ (ë³µì¡í•œ JOIN + ì•Œê³ ë¦¬ì¦˜)

CREATE TABLE campaign_events_source (
  event_type STRING,
  timestamp_field TIMESTAMP(3),
  campaign_id BIGINT,
  advertiser_id BIGINT, 
  product_id BIGINT,
  placement STRING,
  category STRING,
  bid_price DECIMAL(10,2),
  status STRING,
  WATERMARK FOR timestamp_field AS timestamp_field - INTERVAL '30' SECOND
) WITH (
  'connector' = 'kafka',
  'topic' = 'campaign-events',
  'properties.bootstrap.servers' = '${KAFKA_BROKERS}',
  'properties.group.id' = 'flink-rtb-processor',
  'format' = 'json'
);

-- RTB ì ìˆ˜ ê³„ì‚° ë° ìˆœìœ„ ìºì‹œ ìƒì„±
INSERT INTO rtb_ranking_cache
SELECT 
  placement,
  category,
  COLLECT(
    ROW(
      product_id,
      bid_price * COALESCE(pm.ctr, 0.01) + 0.1 * COALESCE(pm.ctr, 0.01) * COALESCE(pm.cvr, 0.005) * pmd.product_price,
      bid_price,
      campaign_id,
      advertiser_id
    ) ORDER BY (bid_price * COALESCE(pm.ctr, 0.01) + 0.1 * COALESCE(pm.ctr, 0.01) * COALESCE(pm.cvr, 0.005) * pmd.product_price) DESC
  ) as ranking_list,
  COUNT(*) as total_campaigns,
  CURRENT_TIMESTAMP as updated_at

FROM campaign_events_source c
LEFT JOIN product_metrics_cache pm ON c.product_id = pm.product_id
LEFT JOIN product_meta_data pmd ON c.product_id = pmd.product_id
WHERE c.event_type IN ('CAMPAIGN_CREATE', 'CAMPAIGN_UPDATE', 'CAMPAIGN_ACTIVATE')
  AND c.status = 'ACTIVE'
GROUP BY c.placement, c.category;
```

### **âœ… Statement 2: PERFORMANCE_AGGREGATOR**

```sql
-- ğŸ“Š ì‹¤ì‹œê°„ ì„±ê³¼ ì§‘ê³„ (5ë¶„ WINDOW + ë³µì¡í•œ ì§‘ê³„)

CREATE TABLE ad_events_source (
  event_type STRING,
  timestamp_field TIMESTAMP(3),
  campaign_id BIGINT,
  advertiser_id BIGINT,
  product_id BIGINT,
  placement STRING,
  category STRING,
  actual_price DECIMAL(10,2),
  revenue_amount DECIMAL(15,2),
  WATERMARK FOR timestamp_field AS timestamp_field - INTERVAL '10' SECOND
) WITH (
  'connector' = 'kafka',
  'topic' = 'ad-events',
  'properties.bootstrap.servers' = '${KAFKA_BROKERS}',
  'properties.group.id' = 'flink-performance-processor',
  'format' = 'json'
);

-- 5ë¶„ ìœˆë„ìš° ì„±ê³¼ ì§‘ê³„
INSERT INTO ad_performance_metrics_realtime
SELECT 
  ROW_NUMBER() OVER (ORDER BY window_start) as id,
  campaign_id,
  product_id,
  placement,
  DATE_FORMAT(window_start, 'yyyy-MM-dd HH:mm:ss') as metric_time,
  
  -- ê¸°ë³¸ ì§€í‘œ
  SUM(CASE WHEN event_type = 'IMPRESSION' THEN 1 ELSE 0 END) as impression_count,
  SUM(CASE WHEN event_type = 'CLICK' THEN 1 ELSE 0 END) as click_count,
  SUM(CASE WHEN event_type = 'CONVERSION' THEN 1 ELSE 0 END) as conversion_count,
  
  -- ë¹„ìš©/ìˆ˜ìµ
  SUM(CASE WHEN event_type = 'CLICK' THEN actual_price ELSE 0 END) as total_spend,
  SUM(CASE WHEN event_type = 'CONVERSION' THEN revenue_amount ELSE 0 END) as total_revenue,
  
  -- ì„±ê³¼ ì§€í‘œ ê³„ì‚°
  CASE 
    WHEN SUM(CASE WHEN event_type = 'IMPRESSION' THEN 1 ELSE 0 END) > 0
    THEN CAST(SUM(CASE WHEN event_type = 'CLICK' THEN 1 ELSE 0 END) AS DOUBLE) 
         / SUM(CASE WHEN event_type = 'IMPRESSION' THEN 1 ELSE 0 END) * 100
    ELSE 0 
  END as ctr,
  
  CASE 
    WHEN SUM(CASE WHEN event_type = 'CLICK' THEN 1 ELSE 0 END) > 0
    THEN CAST(SUM(CASE WHEN event_type = 'CONVERSION' THEN 1 ELSE 0 END) AS DOUBLE) 
         / SUM(CASE WHEN event_type = 'CLICK' THEN 1 ELSE 0 END) * 100
    ELSE 0 
  END as cvr,
  
  CASE 
    WHEN SUM(CASE WHEN event_type = 'CLICK' THEN actual_price ELSE 0 END) > 0
    THEN SUM(CASE WHEN event_type = 'CONVERSION' THEN revenue_amount ELSE 0 END) 
         / SUM(CASE WHEN event_type = 'CLICK' THEN actual_price ELSE 0 END)
    ELSE 0 
  END as roas,
  
  CURRENT_TIMESTAMP as updated_at

FROM TABLE(
  TUMBLE(TABLE ad_events_source, DESCRIPTOR(timestamp_field), INTERVAL '5' MINUTE)
)
WHERE campaign_id IS NOT NULL
GROUP BY campaign_id, product_id, placement, window_start;
```

### **âœ… Statement 3: CONVERSION_MATCHER**

```sql
-- ğŸ”— 24ì‹œê°„ í´ë¦­-ì „í™˜ ë§¤ì¹­ (ë³µì¡í•œ WINDOW + JOIN)

CREATE TABLE conversion_events_source (
  event_type STRING,
  timestamp_field TIMESTAMP(3),
  device_id STRING,
  session_id STRING,
  user_id STRING,
  order_id STRING,
  product_id BIGINT,
  revenue_amount DECIMAL(15,2),
  WATERMARK FOR timestamp_field AS timestamp_field - INTERVAL '1' HOUR
) WITH (
  'connector' = 'kafka',
  'topic' = 'conversion-events',
  'properties.bootstrap.servers' = '${KAFKA_BROKERS}',
  'properties.group.id' = 'flink-conversion-processor',
  'format' = 'json'
);

-- 24ì‹œê°„ ìœˆë„ìš° í´ë¦­-ì „í™˜ ë§¤ì¹­
INSERT INTO conversion_attribution_results
SELECT 
  click_events.ad_id,
  click_events.campaign_id,
  click_events.advertiser_id,
  click_events.product_id,
  conv.order_id,
  conv.revenue_amount,
  TIMESTAMPDIFF(SECOND, click_events.click_time, conv.timestamp_field) as time_to_conversion,
  'LAST_CLICK' as attribution_model,
  click_events.click_time,
  conv.timestamp_field as conversion_time,
  CURRENT_TIMESTAMP as processed_at

FROM (
  -- í´ë¦­ ì´ë²¤íŠ¸ 24ì‹œê°„ ìœˆë„ìš°
  SELECT 
    ad_id,
    campaign_id, 
    advertiser_id,
    product_id,
    device_id,
    session_id,
    timestamp_field as click_time,
    ROW_NUMBER() OVER (
      PARTITION BY device_id, product_id 
      ORDER BY timestamp_field DESC
    ) as click_rank
  FROM TABLE(
    HOP(TABLE ad_events_source, DESCRIPTOR(timestamp_field), INTERVAL '1' HOUR, INTERVAL '24' HOUR)
  )
  WHERE event_type = 'CLICK'
) click_events

INNER JOIN conversion_events_source conv
  ON (conv.device_id = click_events.device_id OR conv.session_id = click_events.session_id)
  AND conv.product_id = click_events.product_id
  AND conv.timestamp_field BETWEEN click_events.click_time AND click_events.click_time + INTERVAL '24' HOUR

WHERE click_events.click_rank = 1  -- Last Click Attribution
  AND conv.event_type = 'CONVERSION';
```

### **âœ… Statement 4: DUPLICATE_DETECTOR â­ ì‹ ê·œ**

```sql
-- ğŸ›¡ï¸ ì¤‘ë³µ í´ë¦­ íƒì§€ ë° ë³´ìƒ ì´ë²¤íŠ¸ ë°œí–‰ (5ë¶„ WINDOW + ë³µì¡í•œ GROUP BY)

CREATE TABLE ad_events_duplicate_source (
  event_type STRING,
  timestamp_field TIMESTAMP(3),
  device_id STRING,
  session_id STRING,
  user_id STRING,
  ad_id STRING,
  campaign_id BIGINT,
  advertiser_id BIGINT,
  product_id BIGINT,
  actual_price DECIMAL(10,2),
  WATERMARK FOR timestamp_field AS timestamp_field - INTERVAL '30' SECOND
) WITH (
  'connector' = 'kafka',
  'topic' = 'ad-events',
  'properties.bootstrap.servers' = '${KAFKA_BROKERS}',
  'properties.group.id' = 'flink-duplicate-detector',
  'format' = 'json'
);

-- ë³´ìƒ ì´ë²¤íŠ¸ ì¶œë ¥ í…Œì´ë¸”
CREATE TABLE compensation_events_sink (
  timestamp_field TIMESTAMP(3),
  advertiser_id BIGINT,
  campaign_id BIGINT,
  ad_id STRING,
  device_id STRING,
  duplicate_click_count BIGINT,
  refund_amount DECIMAL(10,2),
  original_click_timestamps ARRAY<STRING>,
  detection_window STRING,
  reason STRING,
  compensation_type STRING
) WITH (
  'connector' = 'kafka',
  'topic' = 'compensation-events',
  'properties.bootstrap.servers' = '${KAFKA_BROKERS}',
  'format' = 'json'
);

-- 5ë¶„ ìœˆë„ìš° ì¤‘ë³µ í´ë¦­ íƒì§€
INSERT INTO compensation_events_sink
SELECT 
  CURRENT_TIMESTAMP as timestamp_field,
  advertiser_id,
  campaign_id,
  ad_id,
  device_id,
  click_count as duplicate_click_count,
  (click_count - 1) * actual_price as refund_amount,
  click_timestamps as original_click_timestamps,
  '5_MINUTES' as detection_window,
  'DUPLICATE_CLICK_DETECTED' as reason,
  'BALANCE_REFUND' as compensation_type

FROM (
  SELECT 
    advertiser_id,
    campaign_id,
    ad_id,
    device_id,
    actual_price,
    COUNT(*) as click_count,
    COLLECT(DATE_FORMAT(timestamp_field, 'yyyy-MM-dd HH:mm:ss.SSS')) as click_timestamps,
    window_start,
    window_end
    
  FROM TABLE(
    TUMBLE(TABLE ad_events_duplicate_source, DESCRIPTOR(timestamp_field), INTERVAL '5' MINUTE)
  )
  WHERE event_type = 'CLICK'
    AND device_id IS NOT NULL
    AND ad_id IS NOT NULL
  GROUP BY 
    advertiser_id, campaign_id, ad_id, device_id, actual_price, window_start, window_end
  HAVING COUNT(*) > 1  -- ì¤‘ë³µ í´ë¦­ë§Œ í•„í„°ë§
) duplicate_clicks;
```
---

## ğŸ—ï¸ **ìµœì í™”ëœ Terraform êµ¬ì„±**

### **âœ… ë‹¨ì¼ Flink Job (4ê°œ Statement í†µí•©)**

```hcl
# ğŸ¯ Flink Compute Pool (í†µí•© ë¦¬ì†ŒìŠ¤)
resource "confluent_flink_compute_pool" "rtb_processor" {
  display_name = "rtb-stream-processor"
  cloud        = "AWS"
  region       = "us-west-2"
  max_cfu      = 20
  
  environment {
    id = confluent_environment.main.id
  }
}

# ğŸ“Š Statement 1: RTB ìºì‹œ ì—…ë°ì´í„°
resource "confluent_flink_statement" "rtb_cache_updater" {
  compute_pool {
    id = confluent_flink_compute_pool.rtb_processor.id
  }
  principal {
    id = confluent_service_account.flink_runner.id
  }
  
  statement = file("${path.module}/sql/rtb_cache_updater.sql")
  properties = {
    "execution.checkpointing.interval" = "30s"
    "parallelism.default" = "4"
  }
  
  rest_endpoint = confluent_flink_compute_pool.rtb_processor.rest_endpoint
  credentials {
    key    = confluent_api_key.flink_api_key.id
    secret = confluent_api_key.flink_api_key.secret
  }
}

# ğŸ“ˆ Statement 2: ì„±ê³¼ ì§‘ê³„
resource "confluent_flink_statement" "performance_aggregator" {
  compute_pool {
    id = confluent_flink_compute_pool.rtb_processor.id
  }
  principal {
    id = confluent_service_account.flink_runner.id
  }
  
  statement = file("${path.module}/sql/performance_aggregator.sql")
  properties = {
    "execution.checkpointing.interval" = "30s"
    "parallelism.default" = "6"
  }
  
  rest_endpoint = confluent_flink_compute_pool.rtb_processor.rest_endpoint
  credentials {
    key    = confluent_api_key.flink_api_key.id
    secret = confluent_api_key.flink_api_key.secret
  }
  
  depends_on = [confluent_flink_statement.rtb_cache_updater]
}

# ğŸ”— Statement 3: ì „í™˜ ë§¤ì¹­  
resource "confluent_flink_statement" "conversion_matcher" {
  compute_pool {
    id = confluent_flink_compute_pool.rtb_processor.id
  }
  principal {
    id = confluent_service_account.flink_runner.id
  }
  
  statement = file("${path.module}/sql/conversion_matcher.sql")
  properties = {
    "execution.checkpointing.interval" = "60s"
    "parallelism.default" = "4" 
    "table.exec.state.ttl" = "25h"  # 24ì‹œê°„ + 1ì‹œê°„ ë²„í¼
  }
  
  rest_endpoint = confluent_flink_compute_pool.rtb_processor.rest_endpoint
  credentials {
    key    = confluent_api_key.flink_api_key.id
    secret = confluent_api_key.flink_api_key.secret
  }
  
  depends_on = [confluent_flink_statement.performance_aggregator]
}

# ğŸ›¡ï¸ Statement 4: ì¤‘ë³µ íƒì§€ & ë³´ìƒ
resource "confluent_flink_statement" "duplicate_detector" {
  compute_pool {
    id = confluent_flink_compute_pool.rtb_processor.id
  }
  principal {
    id = confluent_service_account.flink_runner.id
  }
  
  statement = file("${path.module}/sql/duplicate_detector.sql")
  properties = {
    "execution.checkpointing.interval" = "30s"
    "parallelism.default" = "4"
    "table.exec.state.ttl" = "6h"  # 5ë¶„ ìœˆë„ìš° + 1ì‹œê°„ ë²„í¼
  }
  
  rest_endpoint = confluent_flink_compute_pool.rtb_processor.rest_endpoint
  credentials {
    key    = confluent_api_key.flink_api_key.id
    secret = confluent_api_key.flink_api_key.secret
  }
  
  depends_on = [confluent_flink_statement.conversion_matcher]
}
```